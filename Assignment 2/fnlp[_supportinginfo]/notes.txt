Comparing results of SRILM with twitter.py on brown

For twitter.py using

    for word in corpus.words():
        if word.isalpha():
            corpus_tokens.append(word.lower())
            #corpus_tokens.append(' ' + word.lower() + ' ') #sgmod
    bigram_model = LgramModel(2,corpus_tokens,True,True)

(q7 from class/fnlp/2017/assignment1_solutions.py)

For SRILM using

            for word in brown.words():
                if word.isalpha():
                    print ' '.join(word.lower())
  > /disk/scratch/tmp/brown_unigrams.txt

and then

 > bin/i686-m64/ngram-count -order 2 -unk -text /disk/scratch/tmp/brown_unigrams.txt -lm /disk/scratch/tmp/brown_2g_wb_u.ngf -wbdiscount1 -wbdiscount2

Results are _almost_ all as good as can be expected given that ngf
only has 7 sig digits.  All unigram probs and some available bigram
probs are correct to 6 places.

All bow[SRILM] = _backoff_alphas[Lgram] to 5 places except
 a) the 7 cases (o, u, i, a, r, n, e) where we have a full set (27) of
    contexts.  These don't matter much, as
    i) backoff will be very rare;
    ii) they are still equal to 2 places
 b) 't', which is only correct to 3 places -- not at all clear why!


And using the model looks good:

  >: echo 't h e' |~/lcontrib_sl6/src/srilm/bin/i686-m64/ngram -lm /disk/scratch/tmp/brown_2g_wb_u.ngf -unk -debug 2 -ppl -
    reading 29 1-grams
    reading 637 2-grams
    t h e
	    p( t | <s> ) 	= [2gram] 0.162078 [ -0.790275 ]
	    p( h | t ...) 	= [2gram] 0.311647 [ -0.506338 ]
	    p( e | h ...) 	= [2gram] 0.483572 [ -0.315539 ]
	    p( </s> | e ...) 	= [2gram] 0.348078 [ -0.458323 ]
    1 sentences, 3 words, 0 OOVs
    0 zeroprobs, logprob= -2.07048 ppl= 3.29321 ppl1= 4.89957

 : log(2**bgm.entropy('the',True,True,verbose=True),10)
    p(t|('',)) = [2-gram] 2.625238
    p(h|('t',)) = [2-gram] 1.682017
    p(e|('h',)) = [2-gram] 1.048198
    p(|('e',)) = [2-gram] 1.522518
    2.0704752802169164
 : [log(2**l,10) for l in [2.625238,1.682017,1.048198,1.522518]]
    [0.7902753837569185,
     0.5063375702167426,
     0.3155390393949937,
     0.45832358693833336]

and with at least one backoff:
  >: echo 'e v b r y' |~/lcontrib_sl6/src/srilm/bin/i686-m64/ngram -lm /disk/scratch/tmp/brown_2g_wb_u.ngf -unk -debug 2 -ppl -
    reading 29 1-grams
    reading 637 2-grams
    e v b r y
	    p( e | <s> ) 	= [2gram] 0.0244891 [ -1.61103 ]
	    p( v | e ...) 	= [2gram] 0.016512 [ -1.7822 ]
	    p( b | v ...) 	= [1gram] 3.40695e-05 [ -4.46763 ]
	    p( r | b ...) 	= [2gram] 0.0572449 [ -1.24226 ]
	    p( y | r ...) 	= [2gram] 0.0299516 [ -1.52358 ]
	    p( </s> | y ...) 	= [2gram] 0.744537 [ -0.128114 ]
    1 sentences, 5 words, 0 OOVs
    0 zeroprobs, logprob= -10.7548 ppl= 62.0112 ppl1= 141.568

  : log(2**bgm.entropy('evbry',True,True,verbose=True),10)
    p(e|('',)) = [2-gram] 5.351715
    p(v|('e',)) = [2-gram] 5.920343
    backing off for ('v', 'b')
     alpha: ('v',) = 0.00268425281311
    p(b|('v',)) = [2-gram] 14.841226
    p(r|('b',)) = [2-gram] 4.126709
    p(y|('r',)) = [2-gram] 5.061224
    p(|('y',)) = [2-gram] 0.425585
    10.754838930601725

  : [log(2**l,10) for l in [5.351715,5.920343,14.841226,4.126709,5.061224,0.425585]]
    [1.611026743244863,
     1.7822008276192813,
     4.467654198428165,
     1.242263192376512,
     1.5235802387744375,
     0.12811385070465542]

Updated automark.py to work with new values and updated
assignment1_solutions.py

Built 3-gram with srilm:

   >: bin/i686-m64/ngram-count -order 3 -unk -text /disk/scratch/tmp/brown_unigrams.txt -lm /disk/scratch/tmp/brown_3g_wb_u.ngf -wbdiscount

Note for future investigation: this does _not_ multi-pad as NLTK code
does, i.e. although e.g. <s> a is in the bigram table, <s> <s> a is
_not_ in the trigram table.  Note how this plays out for entropy:
  >: echo 't h e' |~/lcontrib_sl6/src/srilm/bin/i686-m64/ngram -lm /disk/scratch/tmp/brown_3g_wb_u.ngf -unk -debug 2 -ppl -
    reading 29 1-grams
    reading 637 2-grams
    reading 6247 3-grams
    t h e
	    p( t | <s> ) 	= [2gram] 0.162078 [ -0.790275 ]
	    p( h | t ...) 	= [3gram] 0.689063 [ -0.161741 ]
	    p( e | h ...) 	= [3gram] 0.687527 [ -0.16271 ]
	    p( </s> | e ...) 	= [3gram] 0.676536 [ -0.169709 ]
    1 sentences, 3 words, 0 OOVs
    0 zeroprobs, logprob= -1.28444 ppl= 2.09464 ppl1= 2.68006
The initial bigram is _not_ discounted, i.e. not backed off from a
trigram.  Which makes sense given the way </s> is done.

In _any_ case, really should switch to separate eos and bos tokens.

Starting work on that, in context of moving most updates to ngram.py

Implemented a dump method, to enable easier comparison with srilm

Tried a shorter dataset, just category 'humor':

  > python assignment1_solutions.py -bs >/disk/scratch/tmp/brown_humor_unigrams.txt
  > ngram-count -order 2 -unk -text /disk/scratch/tmp/brown_humor_unigrams.txt -lm /disk/scratch/tmp/brown_2g_wbh_u.ngf -wbdiscount1 -wbdiscount2
versus
  >>> %run assignment1_solutions.py -s
  >>> bgm.dump([/tmp/wbh.ngf],10)
  > tail -n +7 /tmp/wbh.ngf | head -29 > wb_h_u.tsv
  > tail -n +7 /disk/scratch/tmp/brown_2g_wbh_u.ngf| head -29 > /tmp/srilm_h_u.tsv

Explicit unigram logprobabilities are well-aligned:

  >: diff <(cut -f 1,2 srilm_h_u.tsv) <(cut -f 1,2 wb_h_u.tsv)
   3c3
   < -3.557367	<unk>
   ---
   > -3.557333	<unk>

And bigram are basically OK also:

   >  diff <(cut -f 1,2 /disk/scratch/tmp/brown_2g_wbh_u.ngf) <(cut -f 1,2 wbh.ngf)|less

    [all bigram logprob diffs are at worst off by 1 in least sig. digit]

   > paste <(cut -f 2,3 srilm_h_u.tsv) <(cut -f 3 wb_h_u.tsv) | sed '/^\s*$/d;/^<[u/]/d'|while read c x y; do echo -n "$c "; python -c "print $x-$y"; done
     ...
     n 3.2e-05


 >>> bgm.prob('1',('n',),True)
    backing off for ('n', '1')
     alpha: ('n',) = 17.476412556
    Out[26]: 0.0048430493273513

 > echo 'n 1' |~/lcontrib_sl6/src/srilm/bin/i686-m64/ngram -lm /disk/scratch/tmp/brown_2g_wbh_u.ngf  -unk -debug 2 -ppl -
   reading 29 1-grams
   reading 503 2-grams
   n 1
	   p( n | <s> ) 	= [2gram] 0.020673 [ -1.6846 ]
	   p( <unk> | n ...) 	= [1gram] 0.004843031 [ -2.31488 ]
	   p( </s> | <unk> ...) 	= [1gram] 0.182447 [ -0.738863 ]


Tried even smaller (first 1777, then 17, words of humor), and the
differences slowly went away.  In the 1777 case, still true that the
_biggest_ (not v. big) difference was for the only alpha > 1, but this
was no longer for a context with a full set of words_following, or
even the one with the most...

I give up, it does feel like this is _maybe_ just some cumulative
rounding issue in python/nltk vs. c++/srilm.
