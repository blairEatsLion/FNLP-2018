from nltk.corpus.reader.plaintext import PlaintextCorpusReader
from nltk.corpus.reader import RegexpTokenizer
from nltk.tokenize import LineTokenizer
from nltk.corpus.reader.util import read_line_block
from nltkx.model import NgramModel
from nltk import ConditionalFreqDist, ngrams,\
     chain, ConditionalProbDist, WittenBellProbDist, FreqDist
import types

xtwc=PlaintextCorpusReader("/group/ltg/projects/fnlp/",
                          r'2.*\.txt',
                          word_tokenizer=RegexpTokenizer(r'(http|ftp|mailto)://[^\s]+|[\w#@]+|[^\w\s]+'),
                          sent_tokenizer=LineTokenizer(),
                          para_block_reader=read_line_block)

def discount(self):
    return float(self._N)/float(self._N + self._T)

def isclose(a, b, rel_tol=1e-09, abs_tol=0.0):
    # http://stackoverflow.com/a/33024979
    return abs(a-b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)

def check(self):
    totProb=sum(self.prob(sample) for sample in self.samples())
    assert isclose(self.discount(),totProb),\
           "discount %s != totProb %s"%(self.discount(),totProb)
           

WittenBellProbDist.discount = discount
WittenBellProbDist.check = check

def _estimator(fdist, bins):
    """
    Default estimator function using WB.
    """
    # can't be an instance method of NgramModel as they
    # can't be pickled either.
    res=WittenBellProbDist(fdist,fdist.B()+1)
    res.check()
    return res

class LgramModel(NgramModel):
    """
    A processing interface for assigning a probability to the next letter.
    """

    def __init__(self, n, train, pad_left=False, pad_right=False,
                 estimator=None, *estimator_args, **estimator_kwargs):
        """
        Create an ngram language model to capture patterns in n
        consecutive letters of training text.  An estimator smooths
        the probabilities derived from the text and may allow
        generation of ngrams not seen during training.

            >>> from nltk.corpus import brown
            >>> from nltk.probability import LidstoneProbDist
            >>> est = lambda fdist, bins: LidstoneProbDist(fdist, 0.2)
            >>> lm = LgramModel(3, brown.words(categories='news'), estimator=est)
            >>> lm
            <NgramModel with 8716 3-grams>
            >>> lm._backoff
            <NgramModel with 1407 2-grams>
            >>> lm._backoff._model
            <ConditionalProbDist with 78 conditions>
            >>> lm.entropy([l for l in 'banana'])
            3.891611393104264
        :param n: the order of the language model (ngram size)
        :type n: int
        :param train: the training text
        :type train: list(str)
        :param pad_left: whether to pad the left of each sentence with an (n-1)-gram of empty strings
        :type pad_left: bool
        :param pad_right: whether to pad the right of each sentence with an (n-1)-gram of empty strings
        :type pad_right: bool
        :param estimator: a function for generating a probability distribution
        :type estimator: a function that takes a ConditionalFreqDist and
            returns a ConditionalProbDist
        :param estimator_args: Extra arguments for estimator.
            These arguments are usually used to specify extra
            properties for the probability distributions of individual
            conditions, such as the number of bins they contain.
            Note: For backward-compatibility, if no arguments are specified, the
            number of bins in the underlying ConditionalFreqDist are passed to
            the estimator as an argument.
        :type estimator_args: (any)
        :param estimator_kwargs: Extra keyword arguments for the estimator
        :type estimator_kwargs: (any)
        """
        # protection from cryptic behavior for calling programs
        # that use the pre-2.0.2 interface
        assert(isinstance(pad_left, bool))
        assert(isinstance(pad_right, bool))

        self._n = n
        self._W = len(train)
        self._lpad = ('',) * (n - 1) if pad_left else ()
        # Need _rpad even for unigrams or padded entropy will give
        #  wrong answer because '' will be treated as unseen...
        self._rpad = ('',) * (max(1,(n - 1))) if pad_right else ()
        self._padLen = len(self._lpad)+len(self._rpad)

        self._N=0
        delta = 1+self._padLen-n        # len(word)+delta == ngrams in word

        if estimator is None:
            estimator = _estimator

        cfd = ConditionalFreqDist()
        self._ngrams = set()


        ## If given a list of strings, leave it alone :-)
        #if (train is not None) and isinstance(train[0], compat.string_types):
        #    train = [train]
        # Given backoff, a generator isn't acceptable
        if isinstance(train,types.GeneratorType):
          train=list(train)

        assert(isinstance(train[0],types.StringTypes))
        for word in train:
            self._N+=len(word)+delta

            for ngram in ngrams(chain(self._lpad, word, self._rpad), n):
                self._ngrams.add(ngram)
                context = tuple(ngram[:-1])
                token = ngram[-1]
                cfd[context][token]+=1
                # cfd[(context, token)] += 1 See below at _words_following
        #import pdb; pdb.set_trace()
        if not estimator_args and not estimator_kwargs:
            self._model = ConditionalProbDist(cfd, estimator, len(cfd))
        else:
            self._model = ConditionalProbDist(cfd, estimator, *estimator_args, **estimator_kwargs)

        # recursively construct the lower-order models
        if n > 1:
            self._backoff = LgramModel(n-1, train, pad_left, pad_right,
                                       estimator, *estimator_args, **estimator_kwargs)

            # Code below here in this method, and the _words_following and _alpha method, are from
            # http://www.nltk.org/_modules/nltk/model/ngram.html "Last updated on Feb 26, 2015"
            self._backoff_alphas = dict()
            # For each condition (or context)
            #print cfd,cfd.conditions()
            for ctxt in cfd.conditions():
                backoff_ctxt = ctxt[1:]
                backoff_total_pr = 0.0
                total_observed_pr = 0.0

                # this is the subset of words that we OBSERVED following
                # this context.
                # i.e. Count(word | context) > 0
                for word in self._words_following(ctxt, cfd):
                    total_observed_pr += self.prob(word, ctxt)
                    # we also need the total (n-1)-gram probability of
                    # words observed in this n-gram context
                    backoff_total_pr += self._backoff.prob(word, backoff_ctxt)

                assert (0 <= total_observed_pr <= 1),\
                       "sum of probs for %s out of bounds: %s"%(ctxt,total_observed_pr)
                # beta is the remaining probability weight after we factor out
                # the probability of observed words.
                # As a sanity check, both total_observed_pr and backoff_total_pr
                # must be GE 0, since probabilities are never negative
                beta = 1.0 - total_observed_pr

                # if backoff total is 1, that should mean that all samples occur in this context,
                #  so we will never back off.
                # Greater than 1 is an error.
                assert (0 <= backoff_total_pr < 1), \
                       "sum of backoff probs for %s out of bounds: %s"%(ctxt,backoff_total_pr)
                alpha_ctxt = beta / (1.0 - backoff_total_pr)

                self._backoff_alphas[ctxt] = alpha_ctxt

    def _words_following(self, context, cond_freq_dist):
        return cond_freq_dist[context].iterkeys()
        # below from http://www.nltk.org/_modules/nltk/model/ngram.html,
        # depends on new CFD???
        #for ctxt, word in cond_freq_dist.iterkeys():
        #    if ctxt == context:
        #        yield word

    def prob(self, word, context, verbose=False):
        """
        Evaluate the probability of this word (a 1-letter string)
        in this context using Katz Backoff.

        :param word: the letter to get the probability of
        :type word: str
        :param context: the context the letter is in
        :type context: list(str)
        """

        assert(isinstance(word,types.StringTypes))
        assert(len(word) is 1 or word=='')
        context = tuple(context)
        if (context + (word,) in self._ngrams) or (self._n == 1):
            return self[context].prob(word)
        else:
            if verbose:
                print "backing off for %s"%(context+(word,),)
            return (self._alpha(context,verbose) *
                    self._backoff.prob(word, context[1:],verbose))

    def _alpha(self, context,verbose=False):
        """Get the backoff alpha value for the given context
        """
        error_message = "Alphas and backoff are not defined for unigram models"
        assert (not self._n == 1), error_message

        if context in self._backoff_alphas:
            res = self._backoff_alphas[context]
        else:
            res = 1
        if verbose:
            print " alpha: %s = %s"%(context,res)
        return res

    def entropy(self, text, pad_left=False, pad_right=False,
                verbose=False, perItem=False):
        """
        Evaluate the total entropy of a text with respect to the model.
        This is the sum of the log probability of each word in the message.
        """
        # This version takes account of padding for greater accuracy
        if (not(pad_left or pad_right)):
            return super(LgramModel,self).entropy(text,verbose,perItem)
        e = 0.0
        m = len(text)+self._padLen
        for ngram in ngrams(chain(self._lpad, text, self._rpad), self._n):
            context = tuple(ngram[:-1])
            token = ngram[-1]
            cost=self.logprob(token, context, verbose)  # _negative_
                                                        # log2 prob == cost!
            if verbose:
                print "p(%s|%s) = [%s-gram] %7f"%(token,context,self._n,cost)
            e += cost
        if perItem:
            return e/(m-(self._n - 1))
        else:
            return e

#try:
#	from nltk import compat
#except:
#	pass

from nltk.probability import _get_kwarg
try:
    from nltk.probability import islice
except:
    from nltk.util import islice

def plotSorted(self, *args, **kwargs):
        """
        Plot samples from the frequency distribution,
        sorted using a supplied key function.  If an integer
        parameter is supplied, stop after this many samples have been
        plotted.  If two integer parameters m, n are supplied, plot a
        subset of the samples, beginning with m and stopping at n-1.
        For a cumulative plot, specify cumulative=True.
        (Requires Matplotlib to be installed.)

        :param title: The title for the graph
        :type title: str
        :param key: a function to pass to sort to extract the sort key
          given an FD and a sample id.
          Defaults to the value of that sample's entry,
          lambda fd,s:fd[s]
        :type key: function
        :param reverse: True to sort high to low
        :type reverse: bool
        """
        try:
            import pylab
        except ImportError:
            raise ValueError('The plot function requires the matplotlib package (aka pylab). '
                         'See http://matplotlib.sourceforge.net/')

        if len(args) == 0:
            args = [len(self)]

        keyFn = _get_kwarg(kwargs, 'key', lambda fd,s:fd[s])
        reverse = _get_kwarg(kwargs, 'reverse', False)

        samples = list(islice(self, *args))
        samples.sort(key=lambda x:keyFn(self,x),reverse=reverse)

        freqs = [self[sample] for sample in samples]
        ylabel = "Counts"
        # percents = [f * 100 for f in freqs]  only in ProbDist?

        pylab.grid(True, color="silver")
        if not "linewidth" in kwargs:
            kwargs["linewidth"] = 2
        if "title" in kwargs:
            pylab.title(kwargs["title"])
            del kwargs["title"]
        pylab.plot(freqs, **kwargs)
        pylab.xticks(range(len(samples)), [unicode(s) for s in samples], rotation=90)
        pylab.xlabel("Samples")
        pylab.ylabel(ylabel)
        pylab.show()

FreqDist.plotSorted=plotSorted
