Comparing results of SRILM with twitter.py on brown

For twitter.py using

    for word in corpus.words():
        if word.isalpha():
            corpus_tokens.append(word.lower())
            #corpus_tokens.append(' ' + word.lower() + ' ') #sgmod
    bigram_model = LgramModel(2,corpus_tokens,True,True)

(q7 from class/fnlp/2017/assignment1_solutions.py)

For SRILM using

            for word in brown.words():
                if word.isalpha():
                    print ' '.join(word.lower())
  > /disk/scratch/tmp/brown_unigrams.txt

and then

 > bin/i686-m64/ngram-count -order 2 -unk -text /disk/scratch/tmp/brown_unigrams.txt -lm /disk/scratch/tmp/brown_2g_wb_u.ngf -wbdiscount1 -wbdiscount2

Results are _almost_ all as good as can be expected given that ngf
only has 7 sig digits.  All unigram probs and some available bigram
probs are correct to 6 places.

All bow[SRILM] = _backoff_alphas[Lgram] to 5 places except
 a) the 7 cases (o, u, i, a, r, n, e) where we have a full set (27) of
    contexts.  These don't matter much, as
    i) backoff will be very rare;
    ii) they are still equal to 2 places
 b) 't', which is only correct to 3 places -- not at all clear why!


